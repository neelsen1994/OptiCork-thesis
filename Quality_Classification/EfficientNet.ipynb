{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras-vis\n",
    "!git clone https://github.com/WittmannF/LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHEXfp0m0YXy",
    "outputId": "f02da19b-7f48-4130-fd05-3e2e6b784dd4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from LRFinder.keras_callback import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-FFMOke2KA_"
   },
   "source": [
    "### Download & Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "5FuGv1AiKQRZ",
    "outputId": "954746b8-5983-4445-a00d-8734ea0dd9df"
   },
   "outputs": [],
   "source": [
    "# Dataset split ratios\n",
    "np.random.seed(42)\n",
    "VAL_PERC = 0.10\n",
    "TEST_PERC = 0.10\n",
    "TRAIN_PERC = 0.80\n",
    "\n",
    "# Split each folder into train, val and test sets\n",
    "base = pathlib.Path(\"./data/\")\n",
    "base_dirs = [x for x in base.iterdir() if x.is_dir()]\n",
    "labels = [x.stem for x in base.iterdir() if x.is_dir()]\n",
    "base_dirs, labels\n",
    "\n",
    "# Create directory structure\n",
    "dirs = ['train', 'val', 'test']\n",
    "print(dirs)\n",
    "CLASS_LABELS = np.unique(labels)\n",
    "print(CLASS_LABELS)\n",
    "\n",
    "for dirname in dirs:\n",
    "  cur_dir = base / dirname\n",
    "  # If old data exists, delete it and create a new blank directory\n",
    "  if cur_dir.is_dir():\n",
    "    shutil.rmtree(cur_dir)\n",
    "    print(\"DIR DELETED:\", cur_dir)\n",
    "  os.makedirs(cur_dir)\n",
    "  for label in CLASS_LABELS:\n",
    "    os.makedirs(cur_dir / label)\n",
    "    print(\"DIR MADE FOR\", cur_dir / label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9idKZKGKM2q",
    "outputId": "faa7808b-2b3e-45ec-bf94-3606c4a8b7ea"
   },
   "outputs": [],
   "source": [
    "# Create data splits and move files\n",
    "for dirname in base_dirs:\n",
    "  images = [x for x in dirname.iterdir() if x.suffix == \".png\"]\n",
    "  print(images)\n",
    "  labels = len(images) * [dirname.stem]\n",
    "  images, labels = np.array(images), np.array(labels)\n",
    "  # Calculate dataset size for training, validation and test set\n",
    "  train_size = int(len(images) * TRAIN_PERC)\n",
    "  val_size = int(len(images) * VAL_PERC)\n",
    "  test_size = int(len(images) * TEST_PERC)\n",
    "  print(\"[INFO] Split size for: \", dirname, \":\", train_size, val_size, test_size, \n",
    "        (train_size + val_size + test_size))\n",
    "\n",
    "  # Create a random split of files\n",
    "  indices = np.arange(len(labels))\n",
    "  np.random.shuffle(indices)\n",
    "  images_train, labels_train = images[indices[:train_size]], labels[indices[:train_size]]\n",
    "  images_val, labels_val = images[indices[train_size:train_size + val_size]], labels[indices[train_size:train_size + val_size]]\n",
    "  images_test, labels_test = images[indices[train_size + val_size:]], labels[indices[train_size + val_size:]]\n",
    "\n",
    "  # Copy files for each directory\n",
    "  # Train Set\n",
    "  for image, label in zip(images_train, labels_train):\n",
    "    shutil.copyfile(image, base / \"train\" / label / image.name)\n",
    "\n",
    "  # Val Set\n",
    "  for image, label in zip(images_val, labels_val):\n",
    "    shutil.copyfile(image, base / \"val\" / label / image.name)\n",
    "\n",
    "  # Test Set\n",
    "  for image, label in zip(images_test, labels_test):\n",
    "    shutil.copyfile(image, base / \"test\" / label / image.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yorO6NJLG2x",
    "outputId": "dd8840f9-516b-48cb-b256-703f18e6ed9a"
   },
   "outputs": [],
   "source": [
    "# Cleanup old directories\n",
    "for dirname in base_dirs:\n",
    "  # If old data exists, delete it\n",
    "  if dirname.is_dir():\n",
    "    shutil.rmtree(dirname)\n",
    "    print(\"DIR DELETED:\", dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUDhVXU63oJs"
   },
   "source": [
    "### Validate Dataset and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4izfU5DV3rZc"
   },
   "outputs": [],
   "source": [
    "META_SRC = pathlib.Path(\"./data/\")\n",
    "CKPT_DIR = pathlib.Path(\"./model_runs/\")\n",
    "\n",
    "# Params / Hyperparams (Model specific)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 25\n",
    "LEARNING_RATE = 1e-5\n",
    "DROPOUT_RATE = 0.5\n",
    "IMG_HEIGHT = 240\n",
    "IMG_WIDTH = 240\n",
    "NUM_CHANNELS = 3\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1yOXnTM3zNa",
    "outputId": "79d181c3-7061-4bfe-dbeb-19e965950467"
   },
   "outputs": [],
   "source": [
    "# Create image dataset from directory.\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=(META_SRC / \"train\"), labels=\"inferred\", label_mode=\"categorical\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=(META_SRC / \"val\"), labels=\"inferred\", label_mode=\"categorical\", \n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True)\n",
    "print(\"DATASET TYPES: \", type(train_ds), type(val_ds))\n",
    "\n",
    "\"\"\"\n",
    "# Compute steps per epoch for training and validation datasets.\n",
    "compute_steps_per_epoch = lambda x: int(math.ceil(1. * (x / BATCH_SIZE)))\n",
    "# train_steps = compute_steps_per_epoch(tf.size(train_ds))\n",
    "# val_steps = compute_steps_per_epoch(tf.size(val_ds))\n",
    "print(\"DATASET CARDINALITY, TRAIN: {}, VAL: {}\".format(\n",
    "    tf.data.experimental.cardinality(train_ds).numpy(), \n",
    "    tf.data.experimental.cardinality(val_ds).numpy()))\n",
    "train_steps = compute_steps_per_epoch(\n",
    "    tf.data.experimental.cardinality(train_ds).numpy())\n",
    "val_steps = compute_steps_per_epoch(\n",
    "    tf.data.experimental.cardinality(val_ds).numpy())\n",
    "print(\"STEPS PER EPOCH, TRAIN: {}, VAL: {}\".format(train_steps, val_steps))\n",
    "\"\"\"\n",
    "\n",
    "# Prefetch the dataset for improved performance.\n",
    "train_ds.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_ds.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Define Image Augmentation Strategies\n",
    "img_augmentation = tf.keras.models.Sequential(\n",
    "  [\n",
    "   tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.05),\n",
    "   tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal\")\n",
    "  ],\n",
    "  name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "270xPujP4jE4",
    "outputId": "23c3a8c3-fdfd-4232-c520-42626b8c96b9"
   },
   "outputs": [],
   "source": [
    "# View Image Samples\n",
    "plt.figure(figsize=(10, 10))\n",
    "for (image, label) in train_ds.take(1):\n",
    "  for i in range(0, 9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(\"{}\".format(label[i]))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "jDziRix64gaX",
    "outputId": "44b8c981-9bb8-423f-8653-429d8866b675"
   },
   "outputs": [],
   "source": [
    "# Visualize augmentation results\n",
    "for images, labels in train_ds.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = images[4]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = img_augmentation(\n",
    "            tf.expand_dims(first_image, 0), training=True\n",
    "        )\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V5VSPcT8OAI"
   },
   "source": [
    "### Build **NW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PdKTaqM8Rh1"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class HouseDetectorNetwork:\n",
    "    def __init__(self, network):\n",
    "        # Define logger.\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        # self.logger = config.start_logging_to_stdout(self.logger)\n",
    "        # self.logger = config.start_logging_to_file(self.logger)\n",
    "\n",
    "        # Define EfficientNet options.\n",
    "        self.image_res_list = [224, 240, 260, 300, 380, 456, 528, 600]\n",
    "        self.network_list = [\n",
    "            \"EfficientNetB0\",\n",
    "            \"EfficientNetB1\",\n",
    "            \"EfficientNetB2\",\n",
    "            \"EfficientNetB3\",\n",
    "            \"EfficientNetB4\",\n",
    "            \"EfficientNetB5\",\n",
    "            \"EfficientNetB6\",\n",
    "            \"EfficientNetB7\",\n",
    "        ]\n",
    "        self.levels_list = [\n",
    "            [\"top\"],\n",
    "            [\"top\", \"block7\"],\n",
    "            [\"top\", \"block7\", \"block6\"],\n",
    "            [\"top\", \"block7\", \"block6\", \"block5\"],\n",
    "            [\"top\", \"block7\", \"block6\", \"block5\", \"block4\"],\n",
    "        ]\n",
    "\n",
    "        # Load config/environment variables.\n",
    "        # self.ds_path = pathlib.Path(os.getenv(\"DATASET_PATH\"))\n",
    "        # self.num_classes = os.getenv(\"DATASET_NUM_CLASSES\")\n",
    "        self.ds_path = pathlib.Path(\"./data\")\n",
    "        timestr = time.strftime(\"%Y%m%d\")\n",
    "        self.ckpt_path = pathlib.Path(\"./model_runs\") / timestr\n",
    "        self.ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "        label_list = [[\"0\",\"1\",\"2\",\"3\",\"4\"]]\n",
    "        self.ds_labels = label_list[0]\n",
    "        self.num_classes = len(self.ds_labels)\n",
    "\n",
    "        # Define network.\n",
    "        # network_index = int(os.getenv(\"NETWORK_INDEX\"))\n",
    "        self.model_name = self.network_list[network]\n",
    "        self.IMG_HEIGHT = self.IMG_WIDTH = self.image_res_list[network]\n",
    "\n",
    "        # Load default hyperparameters.\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.NUM_EPOCHS = 50\n",
    "        self.LEARNING_RATE = 1e-5\n",
    "        self.DROPOUT_RATE = 0.5\n",
    "        self.NUM_CHANNELS = 3\n",
    "        self.INPUT_SHAPE = (self.IMG_HEIGHT, self.IMG_WIDTH, self.NUM_CHANNELS)\n",
    "\n",
    "        # model placeholder.\n",
    "        self.model = None\n",
    "\n",
    "    def make_datasets(self, type):\n",
    "        print(self.ds_path / type)\n",
    "        dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory=(self.ds_path / type),\n",
    "            labels=\"inferred\",\n",
    "            label_mode=\"categorical\",\n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            image_size=(self.IMG_HEIGHT, self.IMG_WIDTH),\n",
    "            shuffle=True,\n",
    "        )\n",
    "        dataset.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def get_aug(self):\n",
    "        # Define Image Augmentation Strategies\n",
    "        img_augmentation = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.05),\n",
    "                tf.keras.layers.experimental.preprocessing.RandomFlip(\n",
    "                    mode=\"horizontal\"\n",
    "                ),\n",
    "            ],\n",
    "            name=\"img_augmentation\",\n",
    "        )\n",
    "        print(\"Image Augmentation strategy defined.\")\n",
    "        return img_augmentation\n",
    "\n",
    "    def build_and_compile_model(self, ft=True, levels=1):\n",
    "        # Define model inputs.\n",
    "        inputs = tf.keras.layers.Input(self.INPUT_SHAPE)\n",
    "        # Apply data augmentation to inputs.\n",
    "        x = self.get_aug()(inputs)\n",
    "\n",
    "        # Load base model from Keras Applications.\n",
    "        base_model = getattr(tf.keras.applications, self.model_name)(\n",
    "            include_top=False, input_tensor=x, weights=\"imagenet\"\n",
    "        )\n",
    "        # Freeze base model.\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Unfreeze parts of model.\n",
    "        if ft:\n",
    "            # Set trainable layers, but keep batch norm layers frozen.\n",
    "            for layer in base_model.layers:\n",
    "                if all(\n",
    "                    lname not in layer.name for lname in self.levels_list[levels]\n",
    "                ) or isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                    layer.trainable = False\n",
    "                else:\n",
    "                    layer.trainable = True\n",
    "\n",
    "        # Rebuild classifier.\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D(name=\"tl_avgpool\")(base_model.output)\n",
    "        x = tf.keras.layers.Dropout(self.DROPOUT_RATE, name=\"tl_dropout\")(x)\n",
    "        outputs = tf.keras.layers.Dense(\n",
    "            self.num_classes, activation=\"softmax\", name=\"tl_pred\"\n",
    "        )(x)\n",
    "\n",
    "        # Compile model.\n",
    "        model = tf.keras.Model(inputs, outputs, name=self.model_name)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.LEARNING_RATE)\n",
    "        model.compile(\n",
    "            optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        print(\"Model {} successfully compiled.\".format(self.model_name))\n",
    "        self.model = model\n",
    "\n",
    "    def show_trainable_layers(self):\n",
    "        print(\"Trainable layers for model {} are:\".format(self.model_name))\n",
    "        for layer in self.model.layers:\n",
    "            if layer.trainable:\n",
    "                print(\n",
    "                    \"LAYER: {}, TRAINABLE: {}, I/P SHAPE: {}, O/P SHAPE: {}\".format(\n",
    "                        layer.name,\n",
    "                        layer.trainable,\n",
    "                        layer.input_shape,\n",
    "                        layer.output_shape,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def train_model(self, train_ds, val_ds, levels):\n",
    "        checkpoint_name = (\n",
    "            \"house_detector.\"\n",
    "            + self.model_name\n",
    "            + \"-LEV-\"\n",
    "            + str(levels)\n",
    "            + \".weights.{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5\"\n",
    "        )\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=self.ckpt_path / checkpoint_name,\n",
    "            save_weights_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "              # Stop training when `val_loss` is no longer improving\n",
    "              monitor=\"val_loss\",\n",
    "              # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "              min_delta=1e-3,\n",
    "              # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "              patience=3,\n",
    "              verbose=1,\n",
    "        )\n",
    "        callbacks = [model_checkpoint_callback, early_stopping_callback]\n",
    "        print(\n",
    "            \"Starting model training for {} epochs.\".format(self.NUM_EPOCHS)\n",
    "        )\n",
    "        history = self.model.fit(\n",
    "            train_ds,\n",
    "            epochs=self.NUM_EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=val_ds,\n",
    "            verbose=1,\n",
    "        )\n",
    "        print(\"Model training finished.\")\n",
    "        return history\n",
    "\n",
    "    def save_hist_plot(self, hist, metric):\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        plot_name = (\n",
    "            \"house_detector.\"\n",
    "            + self.model_name \n",
    "            + \"-LEV-\"\n",
    "            + str(levels) \n",
    "            + \".\" + metric \n",
    "            + \".\" + timestr + \".png\"\n",
    "        )\n",
    "        plt.plot(hist.history[metric])\n",
    "        plt.plot(hist.history[\"val_\" + metric])\n",
    "        plt.title(\"model \" + metric)\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"best\")\n",
    "        plt.savefig(self.ckpt_path / plot_name, dpi=1000)\n",
    "        print(\"Model plot for {} saved.\".format(metric))\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self, test_ds):\n",
    "        print(\"Starting model evaluation on test set.\")\n",
    "        result = self.model.evaluate(test_ds)\n",
    "        print(dict(zip(self.model.metrics_names, result)))\n",
    "\n",
    "        y_true = list()\n",
    "        y_pred = list()\n",
    "        for i, l in test_ds.unbatch():\n",
    "            y_pred.append(self.model.predict(np.expand_dims(i, axis=0)))\n",
    "            y_true.append(l.numpy())\n",
    "\n",
    "        for i in range(len(self.ds_labels)):\n",
    "            self.show_per_class_accuracy(y_true, y_pred, i)\n",
    "\n",
    "    def show_per_class_accuracy(self, y_true, y_pred, class_num):\n",
    "        cnt = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            yt, yp = np.argmax(yt), np.argmax(yp)\n",
    "            if yt == class_num == yp:\n",
    "                cnt += 1\n",
    "        print(\n",
    "            \"Accuracy for class {} is {}%\".format(class_num, (cnt / len(y_true)) * 100 * self.num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLMLrqs_-QFb"
   },
   "source": [
    "### Train & Evaluate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 3\n",
    "levels = 0\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network)\n",
    "\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "house_detector_network.show_trainable_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OSv04BrK-wBe",
    "outputId": "eb458fc2-26c6-4f26-ddcb-78319ae21717"
   },
   "outputs": [],
   "source": [
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_0 = history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxpfSZk3KXOm",
    "outputId": "424a67d5-d0da-4f9a-e45c-604deb2904bc"
   },
   "outputs": [],
   "source": [
    "with open(\"./plot_history/b2_LEV0_Net3.json\", \"w\") as f:\n",
    "    json.dump(b1_0.history, f)\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 3\n",
    "levels = 1\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network)\n",
    "\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "house_detector_network.show_trainable_layers()\n",
    "\n",
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_0 = history\n",
    "\n",
    "with open(\"./plot_history/b2_LEV1_Net3.json\", \"w\") as f:\n",
    "    json.dump(b1_0.history, f)\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 3\n",
    "levels = 2\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network)\n",
    "\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "house_detector_network.show_trainable_layers()\n",
    "\n",
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_0 = history\n",
    "\n",
    "with open(\"./plot_history/b2_LEV2_Net3.json\", \"w\") as f:\n",
    "    json.dump(b1_0.history, f)\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 3\n",
    "levels = 3\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network)\n",
    "\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "house_detector_network.show_trainable_layers()\n",
    "\n",
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_0 = history\n",
    "\n",
    "with open(\"./plot_history/b2_LEV3_Net3.json\", \"w\") as f:\n",
    "    json.dump(b1_0.history, f)\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = 3\n",
    "levels = 4\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network)\n",
    "\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "house_detector_network.show_trainable_layers()\n",
    "\n",
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_0 = history\n",
    "\n",
    "with open(\"./plot_history/b2_LEV4_Net3.json\", \"w\") as f:\n",
    "    json.dump(b1_0.history, f)\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wPFDiWN8AdSc",
    "outputId": "8b52f7dc-5d7b-4cd6-cde0-82bcd7ffa68e"
   },
   "outputs": [],
   "source": [
    "network = 1\n",
    "levels = 1\n",
    "ds_num = 0\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network, ds_num)\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "# house_detector_network.show_trainable_layers()\n",
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_1 = history\n",
    "\n",
    "with open(\"/content/b1_1.json\", \"w\") as f:\n",
    "    json.dump(b1_1.history, f)\n",
    "\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puHgcP15Mgax"
   },
   "outputs": [],
   "source": [
    "# b1_1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jv1tGb6n-1GC",
    "outputId": "3f6a65da-e716-473d-84fe-6287a0e3e104"
   },
   "outputs": [],
   "source": [
    "network = 1\n",
    "levels = 2\n",
    "ds_num = 0\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network, ds_num)\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Build House Detector model\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "# Check trainable layers.\n",
    "# house_detector_network.show_trainable_layers()\n",
    "# Train model.\n",
    "history = house_detector_network.train_model(train_ds, val_ds, levels)\n",
    "\n",
    "# Save Accuracy Plots.\n",
    "house_detector_network.save_hist_plot(history, \"accuracy\")\n",
    "house_detector_network.save_hist_plot(history, \"loss\")\n",
    "b1_2 = history\n",
    "\n",
    "with open(\"/content/b1_2.json\", \"w\") as f:\n",
    "    json.dump(b1_2.history, f)\n",
    "\n",
    "# Evaluate model.\n",
    "house_detector_network.evaluate_model(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5f_HhDbMlnL"
   },
   "outputs": [],
   "source": [
    "b1_2.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKN0x2EsFC-s"
   },
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3QVNFEdUpyr"
   },
   "outputs": [],
   "source": [
    "def smooth(scalars, weight):  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_LEV4_Net0=json.load(open('./plot_history/b1_LEV4_Net0.json', \"rb\"))\n",
    "b1_LEV4_Net1=json.load(open('./plot_history/b1_LEV4_Net1.json', \"rb\"))\n",
    "b1_LEV4_Net2=json.load(open('./plot_history/b1_LEV4_Net2.json', \"rb\"))\n",
    "b1_LEV4_Net3=json.load(open('./plot_history/b1_LEV4_Net3.json', \"rb\"))\n",
    "b1_LEV4_Net4=json.load(open('./plot_history/b1_LEV4_Net4.json', \"rb\"))\n",
    "\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "fig.suptitle(\"Training different variants of {}\".format(\"EfficientNet\"))\n",
    "plt.subplots_adjust(top=0.925)\n",
    "\n",
    "axes[0][0].set_title(\"Training Loss\")\n",
    "axes[0][1].set_title(\"Validation Loss\")\n",
    "axes[1][0].set_title(\"Training Accuracy\")\n",
    "axes[1][1].set_title(\"Validation Accuracy\")\n",
    "\n",
    "epochs_3 = [i for i in range(1, len(b1_LEV4_Net3['loss']) + 1)]\n",
    "epochs_4 = [i for i in range(1, len(b1_LEV4_Net4['loss']) + 1)]\n",
    "epochs_2 = [i for i in range(1, len(b1_LEV4_Net2['loss']) + 1)]\n",
    "epochs_1 = [i for i in range(1, len(b1_LEV4_Net1['loss']) + 1)]\n",
    "epochs_0 = [i for i in range(1, len(b1_LEV4_Net0['loss']) + 1)]\n",
    "\n",
    "\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_0, \n",
    "                    y=b1_LEV4_Net0['loss'], \n",
    "                    label=\"B0\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_1, \n",
    "                    y=b1_LEV4_Net1['loss'], \n",
    "                    label=\"B1\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_2, \n",
    "                    y=b1_LEV4_Net2['loss'], \n",
    "                    label=\"B2\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_3, \n",
    "                    y=b1_LEV4_Net3['loss'], \n",
    "                    label=\"B3\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_4, \n",
    "                    y=b1_LEV4_Net4['loss'], \n",
    "                    label=\"B4\")\n",
    "\n",
    "axes[0][0].legend()\n",
    "axes[0][0].set(xlabel=\"Epochs\")\n",
    "axes[0][0].set(ylabel=\"Loss\")\n",
    "\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_0, \n",
    "                    y=b1_LEV4_Net0['val_loss'], \n",
    "                    label=\"B0\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_1, \n",
    "                    y=b1_LEV4_Net1['val_loss'], \n",
    "                    label=\"B1\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_2, \n",
    "                    y=b1_LEV4_Net2['val_loss'], \n",
    "                    label=\"B2\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_3, \n",
    "                    y=b1_LEV4_Net3['val_loss'], \n",
    "                    label=\"B3\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_4, \n",
    "                    y=b1_LEV4_Net4['val_loss'], \n",
    "                    label=\"B4\")\n",
    "\n",
    "\n",
    "axes[0][1].legend()\n",
    "axes[0][1].set(xlabel=\"Epochs\")\n",
    "axes[0][1].set(ylabel=\"Val Loss\")\n",
    "\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_0, \n",
    "                    y=b1_LEV4_Net0['accuracy'], \n",
    "                    label=\"B0\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_1, \n",
    "                    y=b1_LEV4_Net1['accuracy'], \n",
    "                    label=\"B1\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_2, \n",
    "                    y=b1_LEV4_Net2['accuracy'], \n",
    "                    label=\"B2\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_3, \n",
    "                    y=b1_LEV4_Net3['accuracy'], \n",
    "                    label=\"B3\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_4, \n",
    "                    y=b1_LEV4_Net4['accuracy'], \n",
    "                    label=\"B4\")\n",
    "\n",
    "axes[1][0].legend()\n",
    "axes[1][0].set(xlabel=\"Epochs\")\n",
    "axes[1][0].set(ylabel=\"Accuracy\")\n",
    "\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_0, \n",
    "                    y=b1_LEV4_Net0['val_accuracy'], \n",
    "                    label=\"B0\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_1, \n",
    "                    y=b1_LEV4_Net1['val_accuracy'], \n",
    "                    label=\"B1\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_2, \n",
    "                    y=b1_LEV4_Net2['val_accuracy'], \n",
    "                    label=\"B2\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_3, \n",
    "                    y=b1_LEV4_Net3['val_accuracy'], \n",
    "                    label=\"B3\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_4, \n",
    "                    y=b1_LEV4_Net4['val_accuracy'], \n",
    "                    label=\"B4\")\n",
    "\n",
    "axes[1][1].legend()\n",
    "axes[1][1].set(xlabel=\"Epochs\")\n",
    "axes[1][1].set(ylabel=\"Val Accuracy\")\n",
    "\n",
    "plt.savefig('Var_EFFNet.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "6dKNM1dmFElF",
    "outputId": "4305dafe-43a3-487c-de94-be8290b89a99"
   },
   "outputs": [],
   "source": [
    "b2_LEV0_Net3=json.load(open('./plot_history/b2_LEV0_Net3.json', \"rb\"))\n",
    "b2_LEV1_Net3=json.load(open('./plot_history/b2_LEV1_Net3.json', \"rb\"))\n",
    "b2_LEV2_Net3=json.load(open('./plot_history/b2_LEV2_Net3.json', \"rb\"))\n",
    "b2_LEV3_Net3=json.load(open('./plot_history/b2_LEV3_Net3.json', \"rb\"))\n",
    "b2_LEV4_Net3=json.load(open('./plot_history/b2_LEV4_Net3.json', \"rb\"))\n",
    "\n",
    "\n",
    "sns.set_palette(\"muted\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "fig.suptitle(\"Fine-Tuning {}\".format(\"EfficientNetB3\"))\n",
    "plt.subplots_adjust(top=0.925)\n",
    "axes[0][0].set_title(\"Training Loss\")\n",
    "axes[0][1].set_title(\"Validation Loss\")\n",
    "axes[1][0].set_title(\"Training Accuracy\")\n",
    "axes[1][1].set_title(\"Validation Accuracy\")\n",
    "\n",
    "# y_labels = [\"LOSS\", \"VAL_LOSS\", \"ACCU\", \"VAL_ACCU\"]\n",
    "epochs_3 = [i for i in range(1, len(b2_LEV3_Net3['loss']) + 1)]\n",
    "epochs_4 = [i for i in range(1, len(b2_LEV4_Net3['loss']) + 1)]\n",
    "epochs_2 = [i for i in range(1, len(b2_LEV2_Net3['loss']) + 1)]\n",
    "epochs_1 = [i for i in range(1, len(b2_LEV1_Net3['loss']) + 1)]\n",
    "epochs_0 = [i for i in range(1, len(b2_LEV0_Net3['loss']) + 1)]\n",
    "\n",
    "# plot.set_yticklabels(plot.get_yticklabels(), rotation=90, horizontalalignment='center')\n",
    "\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_0, \n",
    "                    y=b2_LEV0_Net3['loss'], \n",
    "                    label=\"TOP\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_1, \n",
    "                    y=b2_LEV1_Net3['loss'], \n",
    "                    label=\"TOP, (B7)\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_2, \n",
    "                    y=b2_LEV2_Net3['loss'], \n",
    "                    label=\"TOP, (B6-B7)\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_3, \n",
    "                    y=b2_LEV3_Net3['loss'], \n",
    "                    label=\"TOP, (B5-B7)\")\n",
    "plot = sns.lineplot(ax=axes[0][0], x=epochs_4, \n",
    "                    y=b2_LEV4_Net3['loss'], \n",
    "                    label=\"TOP, (B4-B7)\")\n",
    "axes[0][0].legend()\n",
    "axes[0][0].set(xlabel=\"Epochs\")\n",
    "axes[0][0].set(ylabel=\"Loss\")\n",
    "\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_0, \n",
    "                    y=b2_LEV0_Net3['val_loss'], \n",
    "                    label=\"TOP\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_1, \n",
    "                    y=b2_LEV1_Net3['val_loss'], \n",
    "                    label=\"TOP, (B7)\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_2, \n",
    "                    y=b2_LEV2_Net3['val_loss'], \n",
    "                    label=\"TOP, (B6-B7)\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_3, \n",
    "                    y=b2_LEV3_Net3['val_loss'], \n",
    "                    label=\"TOP, (B5-B7)\")\n",
    "plot = sns.lineplot(ax=axes[0][1], x=epochs_4, \n",
    "                    y=b2_LEV4_Net3['val_loss'], \n",
    "                    label=\"TOP, (B4-B7)\")\n",
    "axes[0][1].legend()\n",
    "axes[0][1].set(xlabel=\"Epochs\")\n",
    "axes[0][1].set(ylabel=\"Val Loss\")\n",
    "\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_0, \n",
    "                    y=b2_LEV0_Net3['accuracy'], \n",
    "                    label=\"TOP\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_1, \n",
    "                    y=b2_LEV1_Net3['accuracy'], \n",
    "                    label=\"TOP, (B7)\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_2, \n",
    "                    y=b2_LEV2_Net3['accuracy'], \n",
    "                    label=\"TOP, (B6-B7)\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_3, \n",
    "                    y=b2_LEV3_Net3['accuracy'], \n",
    "                    label=\"TOP, (B5-B7)\")\n",
    "plot = sns.lineplot(ax=axes[1][0], x=epochs_4, \n",
    "                    y=b2_LEV4_Net3['accuracy'], \n",
    "                    label=\"TOP, (B4-B7)\")\n",
    "axes[1][0].legend()\n",
    "axes[1][0].set(xlabel=\"Epochs\")\n",
    "axes[1][0].set(ylabel=\"Accuracy\")\n",
    "\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_0, \n",
    "                    y=b2_LEV0_Net3['val_accuracy'], \n",
    "                    label=\"TOP\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_1, \n",
    "                    y=b2_LEV1_Net3['val_accuracy'], \n",
    "                    label=\"TOP, (B7)\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_2, \n",
    "                    y=b2_LEV2_Net3['val_accuracy'], \n",
    "                    label=\"TOP, (B6-B7)\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_3, \n",
    "                    y=b2_LEV3_Net3['val_accuracy'], \n",
    "                    label=\"TOP, (B5-B7)\")\n",
    "plot = sns.lineplot(ax=axes[1][1], x=epochs_4, \n",
    "                    y=b2_LEV4_Net3['val_accuracy'], \n",
    "                    label=\"TOP, (B4-B7)\")\n",
    "axes[1][1].legend()\n",
    "axes[1][1].set(xlabel=\"Epochs\")\n",
    "axes[1][1].set(ylabel=\"Val Accuracy\")\n",
    "plt.savefig(\"./finetuning_EfficientNetB3_layers.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "eIj7szyEXUcp",
    "outputId": "bccd1ec5-d245-4fb3-b100-11c860303673"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "# Annotation Example\n",
    "sns.set_palette(\"muted\")\n",
    "# sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# fig.suptitle(\"Images from Preprocessing Dataset\")\n",
    "# plt.subplots_adjust(top=0.975)\n",
    "plt.tight_layout()\n",
    "axes[0].set_title(\"Label 'building'\")\n",
    "axes[1].set_title(\"Label 'no-building'\")\n",
    "\n",
    "img_h = mpimg.imread('/home/home/house_detector_ds_02/test/house/3IgRzt7pwuiPI-yQz3Sy1Q.jpg')\n",
    "img_nh = mpimg.imread('/home/home/house_detector_ds_02/test/nohouse/_SZfEqKz2qOJVAW4Xd4UdA.jpg')\n",
    "\n",
    "axes[0].imshow(img_h)\n",
    "axes[1].imshow(img_nh)\n",
    "\n",
    "axes[0].xaxis.set_visible(False)\n",
    "axes[0].yaxis.set_visible(False)\n",
    "axes[1].xaxis.set_visible(False)\n",
    "axes[1].yaxis.set_visible(False)\n",
    "\n",
    "plt.savefig(\"/content/03_appen.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3xZD5hQ9vO5"
   },
   "source": [
    "### Try LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "id": "d5HFGvcT9yIv",
    "outputId": "5cb43e69-1a31-44a7-e751-e2c5fe464378"
   },
   "outputs": [],
   "source": [
    "network = 1\n",
    "levels = 1\n",
    "ds_num = 0\n",
    "\n",
    "# Make in memory datasets from images to train the network.\n",
    "house_detector_network = HouseDetectorNetwork(network, ds_num)\n",
    "house_detector_network.build_and_compile_model(True, levels)\n",
    "\n",
    "# Params / Hyperparams (Model specific)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "DROPOUT_RATE = 0.25\n",
    "# IMG_HEIGHT = 240\n",
    "# IMG_WIDTH = 240\n",
    "# NUM_CHANNELS = 3\n",
    "# INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n",
    "\n",
    "train_ds = house_detector_network.make_datasets(\"train\")\n",
    "val_ds = house_detector_network.make_datasets(\"val\")\n",
    "test_ds = house_detector_network.make_datasets(\"test\")\n",
    "\n",
    "# Define Callback\n",
    "lr_finder = LRFinder(min_lr=1e-5, max_lr=1)\n",
    "\n",
    "history = house_detector_network.model.fit(\n",
    "            train_ds,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            callbacks=[lr_finder],\n",
    "            validation_data=val_ds,\n",
    "            verbose=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZDMU8qwgFCB"
   },
   "source": [
    "### Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qI-ai2zYgGTM",
    "outputId": "b3e6d3f2-e4df-4b4e-ddff-4ca1f2e77edd"
   },
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "oQYMlM4tgIqD",
    "outputId": "b4c3283c-5e16-4a17-8fa9-151d84482b0a"
   },
   "outputs": [],
   "source": [
    "print(type(os.path.join(os.getcwd(), \"pokesh/path\")))\n",
    "os.path.dirname(os.path.join(os.getcwd(), \"pokesh/path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NSwuIgulQBU",
    "outputId": "d1970b67-05f6-4c9e-d578-14016b00a59e"
   },
   "outputs": [],
   "source": [
    "dict(zip([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWkOuSkrCm8T"
   },
   "source": [
    "### Use Pretrained Model to Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iRXY-4ECsak",
    "outputId": "5d1963d3-8d5c-4176-a70c-79476c729659"
   },
   "outputs": [],
   "source": [
    "# Load New Dataset\n",
    "!unzip -o '/content/drive/My Drive/anupam/datasets/building_age_network/images/scraped/konstanz_images.zip' -d '/home/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86DmnCw_aSCg",
    "outputId": "7c6ce9f9-a7c3-40e2-d9cb-5b3e2f95da36"
   },
   "outputs": [],
   "source": [
    "KONSTANZ_DIR = pathlib.Path(\"/home/konstanz_images/\")\n",
    "img_width, img_height = IMG_HEIGHT, IMG_WIDTH\n",
    "# Make list of usable files\n",
    "konstanz_images = sorted(KONSTANZ_DIR.rglob(\"*.jpg\"))\n",
    "print(len(konstanz_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cmTGUxekBMb",
    "outputId": "d8c03552-e122-4e66-cc5c-2cf97f4c18a1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_skipped = 0\n",
    "for fpath in konstanz_images:\n",
    "    try:\n",
    "        fobj = open(fpath, \"rb\")\n",
    "        is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "    finally:\n",
    "        fobj.close()\n",
    "\n",
    "    if not is_jfif:\n",
    "        num_skipped += 1\n",
    "        # Delete corrupted image\n",
    "        os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPWgSFboh2gw"
   },
   "outputs": [],
   "source": [
    "# def inferencing_dataset(ds_path, BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH):\n",
    "#     dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#         directory=ds_path,\n",
    "#         batch_size=BATCH_SIZE * 2,\n",
    "#         image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "#         label_mode=None\n",
    "#     )\n",
    "#     dataset.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     return dataset\n",
    "# konstanz_dataset = inferencing_dataset(KONSTANZ_DIR, 32, IMG_HEIGHT, IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mvEyuYpd2yc"
   },
   "outputs": [],
   "source": [
    "def load_test_images(images, img_width, img_height):\n",
    "  test_images = []\n",
    "  for i in images:\n",
    "    img = tf.keras.preprocessing.image.load_img(i, target_size=(img_width, img_height))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    test_images.append(img)\n",
    "  return test_images\n",
    "test_images = load_test_images(konstanz_images, img_width, img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9Js1ZkQ9Y2y",
    "outputId": "1503de14-e498-4a23-9fff-499df429b2dd"
   },
   "outputs": [],
   "source": [
    "network = 2\n",
    "levels = 2\n",
    "ds_num = 0\n",
    "detection_model = HouseDetectorNetwork(network, ds_num)\n",
    "detection_model.build_and_compile_model(True, levels)\n",
    "detection_model.model.load_weights(\"/content/drive/MyDrive/anupam/model_runs/house_detector_runs/20210323/house_detector.EfficientNetB2-LEV-2.weights.04-0.01-1.00.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zBpBG-PCmNa",
    "outputId": "ad546e6c-7a83-4a4e-fdbc-edc3881e797c"
   },
   "outputs": [],
   "source": [
    "# test_ds = detection_model.make_datasets(\"test\")\n",
    "# # Evaluate model.\n",
    "# detection_model.evaluate_model(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF5zsA0BF3Zl"
   },
   "outputs": [],
   "source": [
    "konstanz_preds = detection_model.model.predict(np.vstack(test_images), batch_size=detection_model.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmwG1kJSIp8w"
   },
   "outputs": [],
   "source": [
    "savepreds = {}\n",
    "for i, p in zip(konstanz_images, konstanz_preds):\n",
    "    savepreds[i.name] = \"house\" if np.argmax(p) == 0 else \"nohouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6VV0WcCnuXL"
   },
   "outputs": [],
   "source": [
    "# Save predictions to file.\n",
    "with open(\"/content/drive/My Drive/anupam/datasets/building_age_network/images/scraped/konstanz_preds.json\", \"w\") as f:\n",
    "  json.dump(savepreds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5QutM5Mpu2-",
    "outputId": "92f6f236-5e5d-4f85-c490-356648e645a1"
   },
   "outputs": [],
   "source": [
    "savepreds"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "u3xZD5hQ9vO5",
    "rZDMU8qwgFCB",
    "yWkOuSkrCm8T"
   ],
   "name": "house_detection_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
